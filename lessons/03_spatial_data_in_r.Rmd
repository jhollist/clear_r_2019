
```{r setup, echo=FALSE, warning=FALSE, purl=FALSE, message=FALSE}
library(knitr)
options(repos="http://cran.rstudio.com/")
opts_chunk$set(fig.path="figures/",R.options=list(max.print=100),message = FALSE,
               warning = FALSE, error = FALSE)


# Still Masking...
if(!requireNamespace("sf")){
  install.packages("sf")
}
if(!requireNamespace("mapview")){
  install.packages("mapview")
}
#if(!requireNamespace("raster")){
#  install.packages("raster")
#}
if(!requireNamespace("ggplot2")){
  install.packages("ggplot2")
}
if(!requireNamespace("dplyr")){
  install.packages("dplyr")
}
if(!requireNamespace("tidyr")){
  install.packages("tidyr")
}

#library("raster")
library("mapview")
library("ggplot2")
library("dplyr")
library("tidyr")
library("readr")
library("readxl")
library("sf")

```

# Spatial Data in R

In this lesson we will cover the basics of spatial data in R and will do so from an opinionated viewpoint via the `sf` package. 

## Lesson Outline
- [Data in R: The data frame](#data-in-r-the-data-frame)
- [Reading in tabular data](#reading-in-tabular-data)
- [Reading in spatial data](#reading-in-spatial-data)

## Exercises
- [Excercise 3.1](#exercise-31)
- [Excercise 3.2](#exercise-32)

## Data in R: The data frame

Simply put, a data structure is a way for programming languages to handle storing information.  Like most languages, R has several structures (vectors, matrix, lists, etc.).  But R was originally built for data analysis, so the data frame, a spreadsheet like structure with rows and columns, is the most widely used and useful to learn first.  In addition, the data frame (or is it data.frame) is the basis for many modern R pacakges (e.g. the tidyverse) and getting used to it will allow you to quickly build your R skills.

*Note:* It is useful to know more about the different data structures such as vectors, lists, and factors (a weird one that is for catergorical data).  But that is beyond what we have time for.  The best source on this information, I think, is Hadley Wickham's [Data Structures Chapter in Advanced R](http://adv-r.had.co.nz/Data-structures.html).

### Build a data frame
Best way to learn what a data frame is is to look at one.  Let's now build a simple data frame from scratch with the `data.frame()` function.  This is mostly a teaching excercise as we will use the function very little in the excercises to come.  

```{r dataframe}
# Our first data frame

my_df <- data.frame(names = c("joe","jenny","bob","sue"), 
                    age = c(45, 27, 38,51), 
                    knows_r = c(FALSE, TRUE, TRUE,FALSE))
my_df
```

That created a data frame with 3 columns (names, age, knows_r) and four rows.  For each row we have some information on the name of an individual (stored as a character/string), their age (stored as a numeric value), and a column indicating if they know R or not (stored as a boolean/logical).

If you've worked with data before in a spreadsheet or from a table in a database, this rectangular structure should look somewhat familiar.   One way (there are many!) we can access the different parts of the data frame is like:

```{r df_parts}
# Use the dollar sign to get a column
my_df$age

# Grab a row with indexing
my_df[2,]
```

At this point, we have:

- built a data frame from scratch
- seen rows and columns
- heard about "rectangular" structure
- seen how to get a row and a column

The purpose of all this was to introduce the concept of the data frame.  Moving forward we will use other tools to read in data, but the end result will be the same: a data frame with rows (i.e. observations) and columns (i.e. variables).

## Reading in tabular data

Completely creating a data frame from scratch is useful (especially when you start writing your own functions), but more often than not data is stored in an external file that you need to read into R.  These may be delimited text files, spreadsheets, relational databases, SAS files ...  You get the idea.  Instead of treating this subject exhaustively, we will focus just on a single file type, the `.csv` file, that is very commonly encountered and (usually) easy to create from other file types.  For this, we will use the Tidyverse way to do this and use  `read_csv()` from the `readr` pacakge.

The `read_csv()` function is a re-imagined version of the base R fucntion, `read.csv()`.  This command assumes a header row with column names and that the delimiter is a comma. The expected no data value is NA and by default, strings are NOT converted to factors.  This is a big benefit to using `read_csv()` as opposed to `read.csv()`.  Additionally, `read_csv()` has some performance enhancements that make it preferrable when working with larger data sets.  In my limited experience it is about 35% faster than the base R options.  For instance a ~200 MB file with six columns and a five hundred thousand rows took ~6 seconds to read in with `read_csv()` and about 9 seconds with `read.csv()`.  As a comparison, Excel very helpfully (NOT) did not load all of the file!  My default is to use `read_csv()` since it is faster than base but also does a really good job at guessing data types (i.e. your dates will be correct!).  Now if you have a lot of data to read in then even `read_csv()` will be a bit slow.  The best option in this case is to use the `fread()` function from the `data.table` package.  It is lightning fast.  That same ~200MB took less than a second to read in!  You may, however, need to do a bit of additional formating when using `fread()` to get all your columns as you want them.

Source files for `read_csv()` can either be on a local hard drive or, and this is pretty cool, on the web. We will be using the former for our examples and exercises. If you had a file available from a URL it would be accessed like `mydf <- read.csv("https://example.com/my_cool_file.csv")`. As an aside, paths and the use of forward vs back slash is important. R is looking for forward slashes ("/"), or unix-like paths. You can use these in place of the back slash and be fine. You can use a back slash but it needs to be a double back slash ("\\\\"). This is becuase the single backslash in an escape character that is used to indicate things like newlines or tabs. 

For today's workshop we will focus on both grabbing data from a local file and from a URL, we already have an example of this in our `nla_analysis.R`.  In that file look for the line where we use `read_csv()`

For your convenience, it looks like:

```{r read_csv, message=FALSE}
nla_wq_all <- read_csv("nla2007_chemical_conditionestimates_20091123.csv")
```

Run that code, and then we can take a look at our data frame:

```{r look_at}
nla_wq_all
```

### Other ways to read in data

There are many ways to read in data with R.  If you have questions about this, please let Jeff know.  He's happy to chat more about it.  Before we move on though, I will show an example of one other way we can do this.   Since Excel spreadsheets are so ubiquitous we need a reliable way to read in data stored in an excel spreadsheet.  There are a variety of packages that provide this capability, but by far the best (IMHO) is `readxl`.  This is how we read in an File:

```{r readxl}
# You'll very likely need to install it first!!!  How'd we do that?
# also you'll need to download the file
# 
library(readxl)
nla_wq_2007 <- read_excel("nla2007_wq.xlsx")
```

This is the simplest case, but lets dig into the options to see what's possible

```{r echo = FALSE}
args(read_excel)
```

## Reading in spatial data

We have seen multiple ways to read in tabular data into a data frame in R.  This is great but that is only one side of the coin when we are working with spatial data.  We need some special data structures to handle the wide array of spatial data types.  There are two main packages that allow us to read, write, and analyze spatial data in R, the `sf` and `raster` packages

Let's dig in a bit deeper into these two packages.

### sf
The [`sf` package](http://r-spatial.github.io/sf/) provides vector data handling via the Simple Features standard, an Open Geospatial Consortium and International Standards Organization standard for spatial data. In addition, `sf` provides a tidy spatial data format that allows for manipulation with the popular `dplyr` package and when we look at these objects in R, they look a LOT like a data frame, becuase they are.  They do have a special "list column" that handles the spatial data.

Getting `sf` added is no different than adding any other package that is on CRAN.

```{r add_sp, eval=FALSE}
install.packages("sf")
library("sf")
```

### raster

For our raster data processing we will use the venerable `raster` package.   

To install, do: 

```{r add_raster, eval=FALSE}
install.packages("raster")
library("raster")
```

### rgdal and sp

While we won't be using the `rgdal` and `sp` packages directly, `raster` does depend on them so I wanted to mention it breifly. The `rgdal` package provides tools for reading and writing multiple spatial data formats.  It is based on the [Geospatial Data Abstraction Library (GDAL)](http://www.gdal.org/) which is a project of the Open Source Geospatial Foundation (OSGeo).  The `sp` package has been the *de-facto* package for spatial data handling and is required for us to use the `raster` package. 

As before, nothing special to get set up with `rgdal` or `sp` on windows. 

```{r add_rgdal, eval=FALSE}
install.packages("rgdal")
library("rgdal")

install.packages("sp")
libaray("rgdal")
```

Getting set up on Linux or Mac requires more effort (i.e. need to have GDAL installed).  As this is for a USEPA audience the windows installs will work for most.  Thus, discussion of this is mostly beyond the scope of this workshop.  

## Exercise 3.1

The first exercise won't be too thrilling, but we need to make sure everyone has the five packages installed. 

1. Install `sf` and load `sf` into your library.
2. Repeat, with `raster` `sp`, and `rgdal`.

## Reading in data - tabular to spatial

We saw this example in the original script but we will show it again as it is a pretty common way for people to have spatial data.

We read this in above, but to review:

```{r read_in_tab, message=FALSE}
# It probably is, but make sure readr is loaded via library(readr)
nla_wq_all <- read_csv("nla2007_chemical_conditionestimates_20091123.csv")
```

And then we can use some `dplyr` tricks to clean this up.

```{r}
# Make sure dplyr is loaded
# Rename columns to something Jeff approved
nla_wq_rn <- rename_all(nla_wq_all, tolower)
nla_wq_filt <- filter(nla_wq_rn, 
                      site_type == "PROB_Lake",
                      visit_no == 1)
nla_wq_sel <- select(nla_wq_filt, site_id, lon_dd, lat_dd, chla, ntl, ptl)
dd_prj4 <- "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"
nla_wq_sf <- st_as_sf(nla_wq_sel, coords = c("lon_dd", "lat_dd"), crs = dd_prj4)

# Write it out to a shapefile
st_write(nla_wq_sf, "nla_wq.shp", delete_dsn = TRUE)

# Plot it
plot(st_geometry(nla_wq_sf))
```

Now this is a more tradtional way to write this code.  Often now you will see pipes used.  A pipe in R looks like `%>%` and allows us to chain code together.  So, if we wanted to create the `nla_wq_sf` file from above with piped code, it could look like:

```{r eval=FALSE}
# This is a data frame and does not have a coordinate reference system
# We can use PROJ strings for this
dd_prj4 <- "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"

nla_wq_s <- nla_wq_all %>%
  rename_all(tolower) %>%
  filter(site_type == "PROB_Lake",
         visit_no == 1) %>%
  select(site_id, lon_dd, lat_dd, chla, ntl, ptl) %>%
  st_as_sf(coords = c("lon_dd", "lat_dd"), crs = dd_prj4)
```

## Reading in data - spatial formats

### Vector

While this will work well for point data, it is more likely that people will have other vector features (lines, polygons, etc.) in an existing spatial file format, such as shapefiles, geopackage, or file geodatabase.  In these cases we can use a different `sf` function, `st_read` to read in those files.  We will show an example here of working with a shapefile.  

```{r read_shape}
ct_ri_wbd <- st_read("ct_ri_wbd.shp")
plot(st_geometry(ct_ri_wbd))
```

This same approach will work for nearly all vector data, with a few changes depending on the source.  For instance, if we had a file geodatabase named `nhdplus.gdb`, and it contained a layer named `waterbodies` would could access that like this:

```{r gdb, eval=FALSE}
waterbodies <- st_read(dsn = "nhdplus.gdb", layer = "waterbodies")
```

In this example, I am explicitly specifying my arguments so that it is a bit more clear.

### Raster 

We can also work with raster data in R, but will need to use the aptly named `raster` pacakge.   The data should have been extracted from the zip file we have already downloaded.

```{r rast}
# Add the raster package and read raster file in
library(raster)
ct_ri_dem <- raster("ct_ri_elev.tif")
# Plot it
plot(ct_ri_dem)
plot(st_geometry(ct_ri_wbd), add = TRUE)
```

## Exercise 3.2

Now lets make sure we can all read this data in to R.

1. The shapefile is available from <https://github.com/jhollist/clear_r_2019/raw/master/lessons/ct_ri_data.zip>.  Download and extract it.
2. Read in the "ct_ri_wbd.shp" file into an object named `ct_ri_wbd`
3. Read in the "ct_ri_elev.tif" file into an object named `ct_ri_dem`
